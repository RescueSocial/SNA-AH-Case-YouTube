## folders and files map

> __Account info__ | folder: contains scraped account info JSON files

> __videos comments info --old__ | folder: contains the scraped JSON files but only 20 comment for each video

> __videos_data__ | folder: contains scraped JSON files for videos (videos source is given by Christina)

> __Videos_details__ | folder: contains the combined data of videos data folder.

> __Account informations scraping.ipynb__ | file: the jupyter notebook for scraping account info.

> __comment json scraping.ipynb__ | file: the jupyter notebook for scraping comments in JSON.

> __comments_scraping.ipynb__ | file: the jupyter notebook for scraping comments in CSV.

> __channels_id.csv__ | file: the channels IDs for collect account info (this IDs collecting from old comments file.

> __total.csv__ | file: all videos scraped IDs, I use it to prevent duplication before get data in JSON.

> __config.py , helper.py__ | file: helper functions used in jupyter notebooks

> __~$uTube VIDEO OSINT â€“ General Witness Friends Workers.docx__ | file: file contain searching keywords 

### note: video_data and video_detais folders have jupyter notebook inside each used to clean and combine the csv files.

### note: combined files and videos comments info are too large to be on GitHub so it's on Google Drive:
https://www.freelancer.com/users/l.php?url=https:%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1CCavukBqzA-3ImLnKWlYi8oSkY5qxVzy%3Fusp%3Dsharing&sig=0347f2d811d5bfd8cc16ca0f072637e872cb567aee0d2d02bb82fc0f1d0c012c

## Googledrive map:

> __batch1__ : indicate to searches from '1- videos_info for 12 k' to '5- Amber_Heard' 

> __batch2__ : indicate to searches from '6 - Amber Heard' (second time with the same search keyword) to '16- amber heard warner brothers'

I made every folder contain 1000 JSON file max for batch 1 and 500 file for batch 2 this is the reason why folders are multiple 