{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a85169",
   "metadata": {},
   "source": [
    "# Obtaining channels infos by channel ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc42768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from io import StringIO\n",
    "from csv import reader\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ec81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YTstats:\n",
    "    '''\n",
    "    this class takes api_key and channel id\n",
    "    then obtain infor for the channel\n",
    "    '''    \n",
    "  \n",
    "    def __init__(self, api_key, channel_id):\n",
    "        self.api_key = api_key\n",
    "        self.channel_id = channel_id\n",
    "        self.video_statistics = None\n",
    "  \n",
    "    def get_video_statistics(self):\n",
    "        url = f'https://youtube.googleapis.com/youtube/v3/channels?part=snippet&part=contentDetails&part=brandingSettings&part=contentOwnerDetails&part=id&part=localizations&part=statistics&part=status&part=topicDetails&id={self.channel_id}&maxResults=200&key={self.api_key}'\n",
    "        json_url = requests.get(url)\n",
    "        data = json.loads(json_url.text)\n",
    "        nextPageToken = data.get(\"nextPageToken\")\n",
    "        \n",
    "        i = 0\n",
    "        a = ''\n",
    "        if nextPageToken:\n",
    "            while nextPageToken != a:\n",
    "                a = nextPageToken\n",
    "                i += 1\n",
    "                url = f'https://youtube.googleapis.com/youtube/v3/channels?part=snippet&part=contentDetails&part=brandingSettings&part=contentOwnerDetails&part=id&part=localizations&part=statistics&part=status&part=topicDetails&id={self.channel_id}&maxResults=200&key={self.api_key}&pageToken={nextPageToken}'\n",
    "                json_url = requests.get(url)\n",
    "                data1 = json.loads(json_url.text)\n",
    "                nextPageToken = data1.get(\"nextPageToken\")\n",
    "                data['items'] = data['items'] + data1['items']\n",
    "                \n",
    "                if nextPageToken is None:\n",
    "                    break\n",
    "                print('page number:', i+1)\n",
    "              #  print(nextPageToken, len(data['items']))\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            data = data[\"items\"]#[0][\"statistics\"]\n",
    "        except:\n",
    "            data = None\n",
    "  \n",
    "        self.video_statistics = data\n",
    "        return data\n",
    "  \n",
    "    def dump(self):\n",
    "        if self.video_statistics is None:\n",
    "            print('________________nothing happend________________')\n",
    "            return \n",
    "  \n",
    "        video_title = self.channel_id\n",
    "        video_title = video_title.replace(\" \", \"_\")\n",
    "  \n",
    "        # generate a json file with all the statistics data of the youtube video\n",
    "        file_name = video_title + '.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(self.video_statistics, f, indent=4)\n",
    "        print('file dumped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdacb0a",
   "metadata": {},
   "source": [
    "# make sure we will not obtainig channels we already have info for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9525f783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7,475'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read channel IDs for channels we want to obtain:\n",
    "channels_id_new = pd.read_csv(r\"C:\\Users\\Administrator\\AH case\\YouTube\\YouTube analysis\\scraping\\New searches 2022\\channels data\\all_channels_ids.csv\")\n",
    "channels_id_list_new = set(channels_id_new['id'])\n",
    "#channels_id_list_new = set(channels_id_list_new)\n",
    "channels_id_list_new= list(channels_id_list_new)\n",
    "f\"{len(channels_id_list_new):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf77c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13750, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels_id_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc02d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of channels we have info for:  7,436\n"
     ]
    }
   ],
   "source": [
    "# read all json files in folder and subfolder:\n",
    "path_to_json = r\"C:\\Users\\Administrator\\AH case\\YouTube\\YouTube analysis\\scraping\\New searches 2022\\channels data\"\n",
    "names = []\n",
    "for root, dirs, files in os.walk(path_to_json):\n",
    "    for name in files:\n",
    "        if name.endswith((\".json\")):\n",
    "            names.append(name.split('.')[0])\n",
    "            full_path = os.path.join(root, name)\n",
    "names = set(names)\n",
    "names = list(names)\n",
    "print('Count of channels we have info for: ', f\"{len(names):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a6884ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates in our list of channels: True\n"
     ]
    }
   ],
   "source": [
    "print(\"No duplicates in our list of channels:\", len(names) == len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3389bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total channels count BEFORE remove duplicates:  7,475\n",
      "Total channels count AFTER  remove duplicates:  39\n",
      "Removed count:  7,436\n"
     ]
    }
   ],
   "source": [
    "# make a new list with data we don't have info for it:\n",
    "print('Total channels count BEFORE remove duplicates: ', f\"{len(channels_id_list_new):,}\")\n",
    "channels_id = (names)+ (channels_id_list_new)\n",
    "channels_id = pd.DataFrame(channels_id, columns=['id'])\n",
    "channels_id = channels_id.drop_duplicates(keep=False)\n",
    "print('Total channels count AFTER  remove duplicates: ', f\"{len(channels_id):,}\")\n",
    "print('Removed count: ', f\"{len(channels_id_list_new)-len(channels_id):,}\")\n",
    "channels_id = list(channels_id['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fd3c6",
   "metadata": {},
   "source": [
    "# obtaining Data for CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93042770",
   "metadata": {},
   "source": [
    "channels_id = channels_id_list_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be26a9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{len(channels_id):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5e16b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-16 15:32:56.158082\n",
      "UC-TH1SwtlwRekg3IycFPQdQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 38\n",
      "Channels scraped this run: 1\n",
      "UCf2vggg3bZ2LyhIcdP-tHvg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 37\n",
      "Channels scraped this run: 2\n",
      "UCwwSicABdHlgPCVrZSVez5g\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 36\n",
      "Channels scraped this run: 3\n",
      "UCWmh_Cibo9_Kdg-LpQuLfeg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 35\n",
      "Channels scraped this run: 4\n",
      "UClwg29ykdEi1sPXIsWfNX-Q\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 34\n",
      "Channels scraped this run: 5\n",
      "UCnwuJA7pkF26biZOR5g24CQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 33\n",
      "Channels scraped this run: 6\n",
      "UCvCWaUDuh9wzE0XqvbrYFSw\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 32\n",
      "Channels scraped this run: 7\n",
      "UC51A_kgjSjw0sAWIImIh6dg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 31\n",
      "Channels scraped this run: 8\n",
      "UCIfBqGd1xv8jeFyIBi3v5tQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 30\n",
      "Channels scraped this run: 9\n",
      "UCpInwkBdXDo6LDsmQe7km9g\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 29\n",
      "Channels scraped this run: 10\n",
      "UCQn_Knm6RQfYOLpYGkkDCPw\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 28\n",
      "Channels scraped this run: 11\n",
      "UCus3pF8QwCVra6gfzKYU6kA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 27\n",
      "Channels scraped this run: 12\n",
      "UCBZ8ln0Di1U5WubiO7PS7Vg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 26\n",
      "Channels scraped this run: 13\n",
      "UC233fkQamfhRl6qzcsHhRqw\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 25\n",
      "Channels scraped this run: 14\n",
      "UCwkJhEwdh5RkIvLKXC8nDoQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 24\n",
      "Channels scraped this run: 15\n",
      "UCEmOYBjqsSwe9MNEJPPZ3HQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 23\n",
      "Channels scraped this run: 16\n",
      "UC4XAk5aGnzbgzm1XlPt7hxg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 22\n",
      "Channels scraped this run: 17\n",
      "UCsFHwF9t5upAIccloUVxHcw\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 21\n",
      "Channels scraped this run: 18\n",
      "UCKBnZJodEDcoKiSXRnzGqxw\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 20\n",
      "Channels scraped this run: 19\n",
      "UCQzozstXcnlodogtGSFVkjA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 19\n",
      "Channels scraped this run: 20\n",
      "UC9lDXeTqBVNN7zXFk-khbuQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 18\n",
      "Channels scraped this run: 21\n",
      "UCJmE2BiV6fwaqJ-2l_FFDSg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 17\n",
      "Channels scraped this run: 22\n",
      "UCkZlHA7MNE8XnL18RnQ4R_g\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 16\n",
      "Channels scraped this run: 23\n",
      "UCaPuBTFr5KtpeBLFJ4J9JIw\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 15\n",
      "Channels scraped this run: 24\n",
      "UCQY91uX390_E5jyovVIbLxA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 14\n",
      "Channels scraped this run: 25\n",
      "UC8_51OhRPpjm14b5NbS6odA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 13\n",
      "Channels scraped this run: 26\n",
      "UCqlGgiZ2paUWIMDv8tTttEQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 12\n",
      "Channels scraped this run: 27\n",
      "UCfsU1iD08aTYTVfgqQpLpIQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 11\n",
      "Channels scraped this run: 28\n",
      "UCb8DkxU2mucFuToM4Vkl10Q\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 10\n",
      "Channels scraped this run: 29\n",
      "UCGrjyK9065vRZtK3KMVKdIg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 9\n",
      "Channels scraped this run: 30\n",
      "UC-w5wz9Yb4MAKg7A1OC8IKg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 8\n",
      "Channels scraped this run: 31\n",
      "UCXrBDXkUPEZNH1tDFOYv_NA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 7\n",
      "Channels scraped this run: 32\n",
      "UCjxSgg2phqcfY2fo6Hdw5Zg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 6\n",
      "Channels scraped this run: 33\n",
      "UCqqqTe578lvVWWZgUOFquLA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 5\n",
      "Channels scraped this run: 34\n",
      "UCyIzJ641HkmgqUl6_L-26LA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 4\n",
      "Channels scraped this run: 35\n",
      "UCPXw7YNF1wDZ1-0co4mUIqg\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 3\n",
      "Channels scraped this run: 36\n",
      "UCVqkHqGTtsD1B8G1rSyqmmA\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 2\n",
      "Channels scraped this run: 37\n",
      "UC7AJjvfHD5UtBnySpITQ7KQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 1\n",
      "Channels scraped this run: 38\n",
      "UCDW5KMEpTMdywB9KKewA7EQ\n",
      "________________nothing happend________________\n",
      "channels to scrape remain: 0\n",
      "Channels scraped this run: 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06916462182998658"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recive data using API and save it on PC\n",
    " \n",
    "API_KEY1 = \"AIzaSyCQHI5RSbHoKGh6sPPnnOywja7qLS6TXnA\"\n",
    "API_KEY2 = \"AIzaSyA-0KfpLK04NpQN1XghxhSlzG-WkC3DHLs\"\n",
    "API_KEY3 = \"AIzaSyDjJaoIrNIoaIPvX5mTniBHaDNLRGBjpMw\"\n",
    "API_KEY4 = \"AIzaSyB6dfr3l6p5Z7DYhCdXYE8TvCEhsID2qdI\"\n",
    "API_KEY5 = \"AIzaSyAeAJ_SVt-gf8fZ3gorCN0kgX0cTBB_5Ys\"\n",
    "#API_KEY6 = \"AIzaSyAbvbNBsZs3TT2XUFLd-6yBb4cWT43bk0Y\"\n",
    "#API_KEY7 = \"AIzaSyC4BLzJg_D_c2XHHC_926tEeTDbsmAXdyA\"\n",
    "#API_KEY8 = \"AIzaSyAHyQPGuj7n63KcnfKRi4huYuZeyY7XZ_U\"\n",
    "#API_KEY9 = \"AIzaSyDLtBx_yaG3TnlUtVAA2GITtwiVj5mjObM\"\n",
    "#API_KEY10 = \"AIzaSyBfKWvsNpkCQl2EpFevpGHLfc8m3a7Ir9w\"\n",
    "#API_KEY11 = \"AIzaSyCSLhd2GReRwd5iV6cq-x36hGDmfUKOBhw\"\n",
    "#API_KEY12 = \"AIzaSyDU7O1y5qF_F5Po7S24J1z7q-8pyfcS4Bs\"\n",
    "API_KEY13 = \"AIzaSyCrA2GxbVM6Q8Sn1epTQ1twyts2W3n4DPg\"\n",
    "API_KEY14 = \"AIzaSyDB1zv0XoqPGbjE6L_bIKbwrGBkYIqTuS8\"\n",
    "API_KEY15 = \"AIzaSyA3xvloS2Fq4Vb1mHL82pCm6a4lfptVgeI\"\n",
    "API_KEY16 = \"AIzaSyD6wkuqFhYsVUvbMJPn4VrH7YVpyV0_yQ0\"\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "start = time.time()\n",
    "i=0\n",
    "while i < 10000 and len(channels_id) != 0:\n",
    "    for video in channels_id:\n",
    "        i+=1\n",
    "        print(video)\n",
    "        yt = YTstats(API_KEY3, video)\n",
    "        yt.get_video_statistics()\n",
    "        yt.dump()\n",
    "        channels_id.remove(video)\n",
    "        print('channels to scrape remain:', f\"{len(channels_id):,}\")\n",
    "        print('Channels scraped this run:', f\"{i:,}\")\n",
    "        break\n",
    "end = time.time()\n",
    "(end-start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdeb64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('videos_uploader_have_no_data_39u.csv', 'w') as f:\n",
    "    for item in channels_id:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc92e96",
   "metadata": {},
   "source": [
    "# read josn file and combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32231271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7436"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read josn files and make a list with json files\n",
    "# (make 1 change)\n",
    "\n",
    "path_to_json = r'C:\\Users\\Administrator\\AH case\\YouTube\\2022 YouTube data\\JSON- 2022 YouTube data\\Videos uploader data'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "json_files;  \n",
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f099bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000\n",
      "2,000\n",
      "3,000\n",
      "4,000\n",
      "5,000\n",
      "6,000\n",
      "7,000\n",
      "4.351724628607432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7436, 187)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame with all json files:\n",
    "# (make 1 change)\n",
    "start = time.time()\n",
    "i=0\n",
    "df = pd.DataFrame()\n",
    "for file in json_files:\n",
    "    file_path = path_to_json + '/' + file\n",
    "    with open(file_path) as data_file:\n",
    "        i +=1\n",
    "        if i % 1000 == 0: print(f\"{i:,}\")\n",
    "        data = json.load(data_file)\n",
    "        df = df.append(pd.json_normalize(data))\n",
    "end = time.time()\n",
    "print((end-start)/60)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17ad3c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ce8dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7436, 187)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['id'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97731641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns.drop(list(df.filter(regex='localizations')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d54c1df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7436, 41)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "761b183f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['kind', 'etag', 'id', 'snippet.title', 'snippet.description',\n",
       "       'snippet.publishedAt', 'snippet.thumbnails.default.url',\n",
       "       'snippet.thumbnails.default.width', 'snippet.thumbnails.default.height',\n",
       "       'snippet.thumbnails.medium.url', 'snippet.thumbnails.medium.width',\n",
       "       'snippet.thumbnails.medium.height', 'snippet.thumbnails.high.url',\n",
       "       'snippet.thumbnails.high.width', 'snippet.thumbnails.high.height',\n",
       "       'snippet.localized.title', 'snippet.localized.description',\n",
       "       'contentDetails.relatedPlaylists.likes',\n",
       "       'contentDetails.relatedPlaylists.uploads', 'statistics.viewCount',\n",
       "       'statistics.subscriberCount', 'statistics.hiddenSubscriberCount',\n",
       "       'statistics.videoCount', 'status.privacyStatus', 'status.isLinked',\n",
       "       'status.longUploadsStatus', 'brandingSettings.channel.title',\n",
       "       'brandingSettings.channel.description', 'topicDetails.topicIds',\n",
       "       'topicDetails.topicCategories',\n",
       "       'brandingSettings.channel.unsubscribedTrailer',\n",
       "       'brandingSettings.image.bannerExternalUrl',\n",
       "       'brandingSettings.channel.keywords', 'snippet.country',\n",
       "       'brandingSettings.channel.country', 'status.madeForKids',\n",
       "       'snippet.customUrl', 'brandingSettings.channel.moderateComments',\n",
       "       'snippet.defaultLanguage', 'brandingSettings.channel.defaultLanguage',\n",
       "       'brandingSettings.channel.trackingAnalyticsAccountId'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780d230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecf82434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Administrator\\AH case\\YouTube\\YouTube analysis\\scraping\\New searches 2022\\CSV- combined data\\videos uploaders data\\videos_uploader_data_7436v.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1935bd4",
   "metadata": {},
   "source": [
    "emails:\n",
    "\n",
    "some20738@gmail.com\n",
    "\n",
    "someeoonne6@gmail.com\n",
    "\n",
    "some65311@gmail.com\n",
    "\n",
    "ones40370@gmail.com\n",
    "\n",
    "soommeone3@gmail.com\n",
    "\n",
    "someo3315@gmail.com\n",
    "\n",
    "ones45977@gmail.com\n",
    "\n",
    "khairydavid32@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457713ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df192fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare files in a dir with a list then copy the target to the destination:\n",
    "start = time.time()\n",
    "path = r'C:\\Users\\David\\Amber Heard Case\\Youtube\\Data\\channels info\\channels info for commenters - 6 batch 1'\n",
    "source = os.listdir(path)\n",
    "destination = r\"C:\\Users\\David\\Amber Heard Case\\Youtube\\New Data (filtered with 'amber' word)\\channels info for commenters - 5\"\n",
    "\n",
    "i = 0            \n",
    "for file in source:\n",
    "    i+=1\n",
    "    if i % 500 == 0: print(i)\n",
    "    if file.split('.')[0] in channels_id_list_new:\n",
    "        shutil.copy(path + '/'+file, destination)\n",
    "end = time.time()\n",
    "print(\"time in minuts:\", (end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92212c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9635e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd28ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all json files in folder and subfolder:\n",
    "path_to_json = r\"C:\\Users\\David\\Amber Heard Case\\Youtube\\New Data (filtered with 'amber' word)\\channels info\\channels info for commenters - 16\"\n",
    "names = []\n",
    "for root, dirs, files in os.walk(path_to_json):\n",
    "    for name in files:\n",
    "        if name.endswith((\".json\")):\n",
    "            names.append(name.split('.')[0])\n",
    "            full_path = os.path.join(root, name)\n",
    "names = set(names)\n",
    "names = list(names)\n",
    "print('Count of channels we have info for: ', f\"{len(names):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137b08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
