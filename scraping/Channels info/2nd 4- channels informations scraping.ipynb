{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a85169",
   "metadata": {},
   "source": [
    "# Obtaining channels infos by channel ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc42768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from io import StringIO\n",
    "from csv import reader\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ec81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YTstats:\n",
    "    '''\n",
    "    this class takes api_key and channel id\n",
    "    then obtain infor for the channel\n",
    "    '''    \n",
    "  \n",
    "    def __init__(self, api_key, channel_id):\n",
    "        self.api_key = api_key\n",
    "        self.channel_id = channel_id\n",
    "        self.video_statistics = None\n",
    "  \n",
    "    def get_video_statistics(self):\n",
    "        url = f'https://youtube.googleapis.com/youtube/v3/channels?part=snippet&part=contentDetails&part=brandingSettings&part=contentOwnerDetails&part=id&part=localizations&part=statistics&part=status&part=topicDetails&id={self.channel_id}&maxResults=200&key={self.api_key}'\n",
    "        json_url = requests.get(url)\n",
    "        data = json.loads(json_url.text)\n",
    "        nextPageToken = data.get(\"nextPageToken\")\n",
    "        \n",
    "        i = 0\n",
    "        a = ''\n",
    "        if nextPageToken:\n",
    "            while nextPageToken != a:\n",
    "                a = nextPageToken\n",
    "                i += 1\n",
    "                url = f'https://youtube.googleapis.com/youtube/v3/channels?part=snippet&part=contentDetails&part=brandingSettings&part=contentOwnerDetails&part=id&part=localizations&part=statistics&part=status&part=topicDetails&id={self.channel_id}&maxResults=200&key={self.api_key}&pageToken={nextPageToken}'\n",
    "                json_url = requests.get(url)\n",
    "                data1 = json.loads(json_url.text)\n",
    "                nextPageToken = data1.get(\"nextPageToken\")\n",
    "                data['items'] = data['items'] + data1['items']\n",
    "                \n",
    "                if nextPageToken is None:\n",
    "                    break\n",
    "                print('page number:', i+1)\n",
    "              #  print(nextPageToken, len(data['items']))\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            data = data[\"items\"]#[0][\"statistics\"]\n",
    "        except:\n",
    "            data = None\n",
    "  \n",
    "        self.video_statistics = data\n",
    "        return data\n",
    "  \n",
    "    def dump(self):\n",
    "        if self.video_statistics is None:\n",
    "            print('________________nothing happend________________')\n",
    "            return \n",
    "  \n",
    "        video_title = self.channel_id\n",
    "        video_title = video_title.replace(\" \", \"_\")\n",
    "  \n",
    "        # generate a json file with all the statistics data of the youtube video\n",
    "        file_name = video_title + '.json'\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(self.video_statistics, f, indent=4)\n",
    "        print('file dumped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdacb0a",
   "metadata": {},
   "source": [
    "# make sure we will not obtainig channels we already have info for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9525f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read channel IDs for channels we want to obtain:\n",
    "channels_id_new = pd.read_csv(r\"C:\\Users\\David\\Amber Heard Case\\Youtube\\New Data (filtered with 'amber' word)\\2018_channels_ids.csv\")\n",
    "channels_id_list_new = set(channels_id_new['id'])\n",
    "#channels_id_list_new = set(channels_id_list_new)\n",
    "channels_id_list_new= list(channels_id_list_new)\n",
    "f\"{len(channels_id_list_new):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all json files in folder and subfolder:\n",
    "path_to_json = r\"C:\\Users\\David\\Amber Heard Case\\Youtube\\New Data (filtered with 'amber' word)\\channels info\"\n",
    "names = []\n",
    "for root, dirs, files in os.walk(path_to_json):\n",
    "    for name in files:\n",
    "        if name.endswith((\".json\")):\n",
    "            names.append(name.split('.')[0])\n",
    "            full_path = os.path.join(root, name)\n",
    "names = set(names)\n",
    "names = list(names)\n",
    "print('Count of channels we have info for: ', f\"{len(names):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7f5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"No duplicates in our list of channels:\", len(names) == len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ca9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new list with data we don't have info for it:\n",
    "print('Total channels count BEFORE remove duplicates: ', f\"{len(channels_id_list_new):,}\")\n",
    "channels_id = (names)+ (channels_id_list_new)\n",
    "channels_id = pd.DataFrame(channels_id, columns=['id'])\n",
    "channels_id = channels_id.drop_duplicates(keep=False)\n",
    "print('Total channels count AFTER  remove duplicates: ', f\"{len(channels_id):,}\")\n",
    "print('Removed count: ', f\"{len(channels_id_list_new)-len(channels_id):,}\")\n",
    "channels_id = list(channels_id['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fd3c6",
   "metadata": {},
   "source": [
    "# obtaining Data for CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4e39a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_id = channels_id_list_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a76a227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{len(channels_id):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e5e16b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-26 13:49:07.329829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13244/326599385.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannels_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchannels_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# recive data using API and save it on PC\n",
    " \n",
    "API_KEY1 = \"AIzaSyCQHI5RSbHoKGh6sPPnnOywja7qLS6TXnA\"\n",
    "API_KEY2 = \"AIzaSyA-0KfpLK04NpQN1XghxhSlzG-WkC3DHLs\"\n",
    "API_KEY3 = \"AIzaSyDjJaoIrNIoaIPvX5mTniBHaDNLRGBjpMw\"\n",
    "API_KEY4 = \"AIzaSyB6dfr3l6p5Z7DYhCdXYE8TvCEhsID2qdI\"\n",
    "API_KEY5 = \"AIzaSyAeAJ_SVt-gf8fZ3gorCN0kgX0cTBB_5Ys\"\n",
    "#API_KEY6 = \"AIzaSyAbvbNBsZs3TT2XUFLd-6yBb4cWT43bk0Y\"\n",
    "#API_KEY7 = \"AIzaSyC4BLzJg_D_c2XHHC_926tEeTDbsmAXdyA\"\n",
    "#API_KEY8 = \"AIzaSyAHyQPGuj7n63KcnfKRi4huYuZeyY7XZ_U\"\n",
    "#API_KEY9 = \"AIzaSyDLtBx_yaG3TnlUtVAA2GITtwiVj5mjObM\"\n",
    "#API_KEY10 = \"AIzaSyBfKWvsNpkCQl2EpFevpGHLfc8m3a7Ir9w\"\n",
    "#API_KEY11 = \"AIzaSyCSLhd2GReRwd5iV6cq-x36hGDmfUKOBhw\"\n",
    "#API_KEY12 = \"AIzaSyDU7O1y5qF_F5Po7S24J1z7q-8pyfcS4Bs\"\n",
    "API_KEY13 = \"AIzaSyCrA2GxbVM6Q8Sn1epTQ1twyts2W3n4DPg\"\n",
    "API_KEY14 = \"AIzaSyDB1zv0XoqPGbjE6L_bIKbwrGBkYIqTuS8\"\n",
    "API_KEY15 = \"AIzaSyA3xvloS2Fq4Vb1mHL82pCm6a4lfptVgeI\"\n",
    "API_KEY16 = \"AIzaSyD6wkuqFhYsVUvbMJPn4VrH7YVpyV0_yQ0\"\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "start = time.time()\n",
    "i=0\n",
    "while i < 10000 and len(channels_id) != 0:\n",
    "    for video in channels_id[10000:]:\n",
    "        i+=1\n",
    "        print(video)\n",
    "        yt = YTstats(API_KEY14, video)\n",
    "        yt.get_video_statistics()\n",
    "        yt.dump()\n",
    "        channels_id.remove(video)\n",
    "        print('channels to scrape remain:', f\"{len(channels_id):,}\")\n",
    "        print('Channels scraped this run:', f\"{i:,}\")\n",
    "        break\n",
    "end = time.time()\n",
    "(end-start)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc92e96",
   "metadata": {},
   "source": [
    "# read josn file and combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32231271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read josn files and make a list with json files\n",
    "# (make 1 change)\n",
    "\n",
    "path_to_json = r'C:\\Users\\David\\Amber Heard Case\\Youtube\\SNA-AH-Case-YouTube\\scraping\\Channels info\\channels info for commenters\\channels info for commenters - 2 batch 1'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "json_files;  \n",
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f099bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame with all json files:\n",
    "# (make 1 change)\n",
    "i=0\n",
    "df = pd.DataFrame()\n",
    "for file in json_files[14000:14070]:\n",
    "    file_path = path_to_json + '/' + file\n",
    "    with open(file_path) as data_file:\n",
    "        i +=1\n",
    "        print(file_path, i)\n",
    "        data = json.load(data_file)\n",
    "        df = df.append(pd.json_normalize(data))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa410a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df_1.shape, df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc11b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.columns.drop(list(df.filter(regex='thumbnails')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdee0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_head_list = list(df.columns)\n",
    "column_head_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf82434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\David\\Amber Heard Case\\Youtube\\Data\\combined files for channels\\combined accounts info of commenters 2 batch 1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8c129",
   "metadata": {},
   "source": [
    "email: some20738@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea935b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
